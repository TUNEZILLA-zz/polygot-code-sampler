#!/usr/bin/env python3
"""
üéõÔ∏è Polyglot Code Mixer - Production FastAPI Service

Production-ready code generation service with hardening, observability,
rate limiting, and security measures.
"""

import hashlib
import json
import logging
import time
from collections import defaultdict, deque
from datetime import datetime
from functools import lru_cache
from typing import Any, Dict, List, Literal, Optional

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, validator

from pcs.core import PyToIR

# Import the central renderer API
from pcs.renderer_api import render as render_generic

# Configure structured logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Rate limiting storage
rate_limit_storage = defaultdict(lambda: deque(maxlen=100))


# LRU cache for generated code
@lru_cache(maxsize=1000)
def cached_render(target: str, code_hash: str, flags_json: str) -> str:
    """Cache render results for 60 seconds"""
    return None  # Will be set by actual render calls


app = FastAPI(
    title="üéõÔ∏è Polyglot Code Mixer API",
    description="Production-ready code generation service",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
)

# Security middleware
app.add_middleware(
    TrustedHostMiddleware,
    allowed_hosts=["localhost", "127.0.0.1", "*.github.io", "*.yourdomain.com"],
)

# CORS middleware with restricted origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://tunezilla-zz.github.io",
        "http://localhost:8080",
        "http://localhost:3000",
        "http://127.0.0.1:8080",
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
    max_age=3600,
)


# Rate limiting middleware
async def rate_limit_middleware(request: Request, call_next):
    """Simple token bucket rate limiting"""
    client_ip = request.client.host
    now = time.time()

    # Clean old entries (older than 1 minute)
    rate_limit_storage[client_ip] = deque(
        [t for t in rate_limit_storage[client_ip] if now - t < 60], maxlen=100
    )

    # Check rate limit (10 requests per minute)
    if len(rate_limit_storage[client_ip]) >= 10:
        return JSONResponse(
            status_code=429,
            content={"error": "Rate limit exceeded", "retry_after": 60},
            headers={"Retry-After": "60"},
        )

    # Add current request
    rate_limit_storage[client_ip].append(now)

    response = await call_next(request)
    return response


app.middleware("http")(rate_limit_middleware)


# Request size limiting
@app.middleware("http")
async def limit_request_size(request: Request, call_next):
    """Limit request body size to 10KB"""
    if request.method == "POST":
        body = await request.body()
        if len(body) > 10240:  # 10KB
            return JSONResponse(
                status_code=413,
                content={"error": "Request body too large", "max_size": "10KB"},
            )
    return await call_next(request)


# Pydantic models with validation
class RenderRequest(BaseModel):
    """Request model for code generation with validation"""

    code: str = Field(..., description="Python comprehension code", max_length=5000)
    target: Literal["rust", "ts", "go", "csharp", "julia", "sql"] = Field(
        ..., description="Target backend"
    )

    # Mixer-driven flags
    parallel: Optional[bool] = Field(None, description="Enable parallel processing")
    mode: Optional[Literal["auto", "loops", "broadcast"]] = Field(
        None, description="Code generation mode"
    )
    unsafe: Optional[bool] = Field(None, description="Enable unsafe optimizations")
    explain: Optional[bool] = Field(None, description="Include explanatory comments")
    dialect: Optional[Literal["postgresql", "sqlite", "duckdb", "bigquery"]] = Field(
        None, description="SQL dialect"
    )

    # Effects rack toggles
    effects: Dict[str, bool] = Field(
        default_factory=dict, description="Optimization effects"
    )

    # Automation lane settings
    grace_period: Optional[int] = Field(
        None, ge=0, le=10000, description="Grace period in milliseconds"
    )
    regression_threshold: Optional[float] = Field(
        None, ge=0, le=100, description="Regression threshold percentage"
    )
    emergency_override: Optional[bool] = Field(
        None, description="Emergency override flag"
    )

    @validator("code")
    def validate_code(cls, v):
        """Basic validation for Python comprehensions"""
        if not v.strip():
            raise ValueError("Code cannot be empty")
        if len(v) > 5000:
            raise ValueError("Code too long (max 5000 chars)")
        # Basic check for comprehension-like syntax
        if not any(
            keyword in v for keyword in ["for ", "in ", "if ", "sum(", "max(", "min("]
        ):
            raise ValueError(
                "Input must be a Python comprehension or generator expression"
            )
        return v


class RenderResponse(BaseModel):
    """Response model for generated code"""

    ok: bool
    target: str
    code: str
    meta: Dict[str, Any] = Field(default_factory=dict)
    timing: Dict[str, float] = Field(default_factory=dict)


class BatchRenderRequest(BaseModel):
    """Request model for batch rendering"""

    code: str = Field(..., description="Python comprehension code", max_length=5000)
    targets: List[Literal["rust", "ts", "go", "csharp", "julia", "sql"]] = Field(
        ..., description="Target backends"
    )
    parallel: Optional[bool] = Field(None, description="Enable parallel processing")
    mode: Optional[Literal["auto", "loops", "broadcast"]] = Field(
        None, description="Code generation mode"
    )
    unsafe: Optional[bool] = Field(None, description="Enable unsafe optimizations")
    explain: Optional[bool] = Field(None, description="Include explanatory comments")
    effects: Dict[str, bool] = Field(
        default_factory=dict, description="Optimization effects"
    )


class HealthResponse(BaseModel):
    """Health check response"""

    ok: bool
    version: str
    renderer_api_version: str
    policy_sha: str
    commit: str
    uptime_seconds: float
    cache_size: int
    rate_limit_status: Dict[str, int]


# Initialize the parser
parser = PyToIR()
start_time = time.time()


# Get version info
def get_version_info():
    """Get version information for health checks"""
    try:
        import pcs

        renderer_version = getattr(pcs, "__version__", "unknown")
    except:
        renderer_version = "unknown"

    return {
        "version": "1.0.0",
        "renderer_api_version": renderer_version,
        "policy_sha": "abc123",  # Would be actual policy SHA
        "commit": "def456",  # Would be actual git commit
        "uptime_seconds": time.time() - start_time,
    }


@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "service": "üéõÔ∏è Polyglot Code Mixer API",
        "version": "1.0.0",
        "status": "ready",
        "endpoints": {
            "render": "/v1/render",
            "batch": "/v1/render/batch",
            "health": "/v1/health",
            "presets": "/v1/presets",
            "docs": "/docs",
        },
    }


@app.get("/v1/health", response_model=HealthResponse)
async def health():
    """Comprehensive health check"""
    try:
        # Test basic parsing
        test_ir = parser.parse("[x for x in range(5)]")

        version_info = get_version_info()

        return HealthResponse(
            ok=True,
            **version_info,
            cache_size=len(cached_render.cache_info()),
            rate_limit_status={
                "active_ips": len(rate_limit_storage),
                "total_requests": sum(
                    len(requests) for requests in rate_limit_storage.values()
                ),
            },
        )
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=500, detail="Service unhealthy")


@app.post("/v1/render", response_model=RenderResponse)
async def render_code(request: RenderRequest):
    """
    Generate code for a specific backend based on mixer settings.

    Maps audio production metaphors to actual renderer flags:
    - Fader level > 60% ‚Üí parallel=True
    - Fader level > 85% ‚Üí unsafe=True
    - Effects toggles ‚Üí optimization flags
    - Preset ‚Üí mode selection
    """
    start_time = time.time()

    try:
        # Create cache key
        code_hash = hashlib.md5(request.code.encode()).hexdigest()
        flags_json = json.dumps(
            {
                "parallel": request.parallel,
                "mode": request.mode,
                "unsafe": request.unsafe,
                "explain": request.explain,
                "dialect": request.dialect,
                "effects": request.effects,
            },
            sort_keys=True,
        )

        # Check cache first
        cached_result = cached_render(request.target, code_hash, flags_json)
        if cached_result:
            logger.info(f"Cache hit for {request.target}")
            return RenderResponse(
                ok=True,
                target=request.target,
                code=cached_result,
                meta={"cached": True, "effects": request.effects},
                timing={"total_ms": 0},
            )

        logger.info(f"Rendering {request.target} for code: {request.code[:50]}...")

        # Parse Python code to IR
        parse_start = time.time()
        ir = parser.parse(request.code)
        parse_time = time.time() - parse_start

        # Map effects to renderer flags
        render_flags = {
            "parallel": request.parallel,
            "mode": request.mode,
            "unsafe": request.unsafe,
            "explain": request.explain if request.explain is not None else True,
            "dialect": request.dialect,
        }

        # Add effects-based flags
        if request.effects.get("predicate_pushdown", False):
            render_flags["predicate_pushdown"] = True
        if request.effects.get("constant_folding", False):
            render_flags["constant_folding"] = True
        if request.effects.get("parallel_safety", False):
            render_flags["parallel_safety"] = True
        if request.effects.get("type_inference", False):
            render_flags["type_inference"] = True

        # Generate code using the central renderer API
        render_start = time.time()
        code = render_generic(request.target, ir, **render_flags)
        render_time = time.time() - render_start

        total_time = time.time() - start_time

        # Cache the result
        cached_render.cache_clear()  # Clear old entries periodically
        cached_render(request.target, code_hash, flags_json)

        logger.info(
            f"Generated {len(code)} chars for {request.target} in {total_time:.3f}s"
        )

        return RenderResponse(
            ok=True,
            target=request.target,
            code=code,
            meta={
                "effects": request.effects,
                "flags": render_flags,
                "input_length": len(request.code),
                "output_length": len(code),
                "cached": False,
            },
            timing={
                "parse_ms": parse_time * 1000,
                "render_ms": render_time * 1000,
                "total_ms": total_time * 1000,
            },
        )

    except ValueError as e:
        logger.warning(f"Validation error for {request.target}: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid input: {str(e)}")
    except Exception as e:
        logger.error(f"Render failed for {request.target}: {e}")
        raise HTTPException(status_code=500, detail=f"Code generation failed: {str(e)}")


@app.post("/v1/render/batch")
async def render_batch(request: BatchRenderRequest):
    """
    Batch render multiple targets for the same Python code.
    Useful for the mixer interface to generate all active tracks at once.
    """
    start_time = time.time()
    results = []

    for target in request.targets:
        try:
            # Create individual request
            individual_request = RenderRequest(
                code=request.code,
                target=target,
                parallel=request.parallel,
                mode=request.mode,
                unsafe=request.unsafe,
                explain=request.explain,
                effects=request.effects,
            )

            result = await render_code(individual_request)
            results.append(result)
        except Exception as e:
            logger.error(f"Batch render failed for {target}: {e}")
            results.append(
                RenderResponse(
                    ok=False,
                    target=target,
                    code=f"Error: {str(e)}",
                    meta={"error": str(e)},
                    timing={"total_ms": 0},
                )
            )

    total_time = time.time() - start_time

    return {
        "results": results,
        "batch_timing": {"total_ms": total_time * 1000, "count": len(request.targets)},
    }


@app.get("/v1/presets")
async def get_presets():
    """Get available mixer presets with caching"""
    return {
        "presets": {
            "hifi": {
                "name": "üéµ Hi-Fi Sequential",
                "description": "Clean, readable code with minimal optimizations",
                "settings": {
                    "parallel": False,
                    "mode": "loops",
                    "unsafe": False,
                    "explain": True,
                    "effects": {
                        "predicate_pushdown": True,
                        "constant_folding": True,
                        "parallel_safety": True,
                        "type_inference": False,
                    },
                },
            },
            "punchy": {
                "name": "‚ö° Punchy Parallel",
                "description": "Maximum throughput with aggressive parallelization",
                "settings": {
                    "parallel": True,
                    "mode": "loops",
                    "unsafe": True,
                    "explain": False,
                    "effects": {
                        "predicate_pushdown": True,
                        "constant_folding": True,
                        "parallel_safety": True,
                        "type_inference": True,
                    },
                },
            },
            "lofi": {
                "name": "üéß Lo-Fi Minimalist",
                "description": "Simple loops with no abstractions",
                "settings": {
                    "parallel": False,
                    "mode": "loops",
                    "unsafe": False,
                    "explain": False,
                    "effects": {
                        "predicate_pushdown": False,
                        "constant_folding": False,
                        "parallel_safety": False,
                        "type_inference": False,
                    },
                },
            },
            "sql_club": {
                "name": "üéõÔ∏è SQL Club Mix",
                "description": "Heavy use of window functions and optimizations",
                "settings": {
                    "parallel": False,
                    "mode": "loops",
                    "unsafe": False,
                    "explain": True,
                    "dialect": "postgresql",
                    "effects": {
                        "predicate_pushdown": True,
                        "constant_folding": True,
                        "parallel_safety": False,
                        "type_inference": True,
                    },
                },
            },
        }
    }


@app.get("/v1/metrics")
async def metrics():
    """Prometheus-style metrics endpoint"""
    return {
        "requests_total": sum(
            len(requests) for requests in rate_limit_storage.values()
        ),
        "active_connections": len(rate_limit_storage),
        "cache_hits": cached_render.cache_info().hits,
        "cache_misses": cached_render.cache_info().misses,
        "uptime_seconds": time.time() - start_time,
    }


# Error handlers
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Custom error handler with friendly messages"""
    error_messages = {
        400: "Invalid input: Please check your Python comprehension syntax",
        413: "Request too large: Maximum 10KB per request",
        429: "Too many requests: Please wait a moment before trying again",
        500: "Internal server error: Please try again later",
    }

    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": error_messages.get(exc.status_code, exc.detail),
            "status_code": exc.status_code,
            "timestamp": datetime.utcnow().isoformat(),
        },
    )


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "server_prod:app",
        host="0.0.0.0",
        port=8787,
        reload=False,  # Disable reload in production
        log_level="info",
        access_log=True,
    )
