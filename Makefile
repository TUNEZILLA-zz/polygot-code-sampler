# Makefile for Polyglot Code Sampler - Julia Backend Dev Workflow

.PHONY: help test-julia test-fixtures test-differential benchmark-julia clean-julia dev-workflow

# Default target
help:
	@echo "ðŸš€ Polyglot Code Sampler - Julia Backend Dev Workflow"
	@echo "=================================================="
	@echo ""
	@echo "Available targets:"
	@echo "  test-julia      - Run all Julia backend tests"
	@echo "  test-fixtures   - Test IR fixture â†’ 4 goldens regression safety net"
	@echo "  test-differential - Run differential tests (IR â†’ Julia vs reference)"
	@echo "  benchmark-julia - Run Julia performance benchmarks"
	@echo "  dev-workflow    - Complete dev workflow (test + benchmark)"
	@echo "  clean-julia     - Clean Julia test artifacts"
	@echo ""
	@echo "Benchmark targets:"
	@echo "  bench           - Run all backend benchmarks (Julia, Rust, Go, TS, C#)"
	@echo "  bench-agg       - Aggregate benchmark results for dashboard"
	@echo "  bench-publish   - Publish benchmark dashboard to GitHub Pages"
	@echo "  bench-full      - Run benchmarks and aggregate results"
	@echo "  bench-status    - Show benchmark status and results"
	@echo ""

# Test Julia backend
test-julia:
	@echo "ðŸ§ª Running Julia Backend Tests..."
	python3 -m pytest tests/test_one_ir_many_goldens.py -v
	@echo "âœ… Julia backend tests completed"

# Test fixture golden files
test-fixtures:
	@echo "ðŸ§ª Testing IR Fixture â†’ 4 Goldens Regression Safety Net..."
	python3 scripts/test_fixture_goldens.py
	@echo "âœ… Fixture golden tests completed"

# Run differential tests
test-differential:
	@echo "ðŸ§ª Running Differential Tests (IR â†’ Julia vs Reference)..."
	python3 scripts/test_differential.py
	@echo "âœ… Differential tests completed"

# Run Julia performance benchmarks
benchmark-julia:
	@echo "âš¡ Running Julia Performance Benchmarks..."
	@if command -v julia >/dev/null 2>&1; then \
		echo "ðŸ“Š Sequential vs Parallel Performance Comparison:"; \
		julia --project -e 'include("examples/julia/sanity_perf_check.jl")'; \
		echo "âœ… Julia benchmarks completed"; \
	else \
		echo "âš ï¸  Julia not found - skipping benchmarks"; \
		echo "   Install Julia to run performance benchmarks"; \
	fi

# Complete dev workflow
dev-workflow: test-julia test-fixtures test-differential benchmark-julia
	@echo ""
	@echo "ðŸŽ‰ Complete Dev Workflow Completed!"
	@echo "âœ… All tests passed"
	@echo "âœ… Regression safety net intact"
	@echo "âœ… Performance benchmarks completed"
	@echo ""
	@echo "ðŸš€ Julia backend is ready for production!"

# Clean Julia test artifacts
clean-julia:
	@echo "ðŸ§¹ Cleaning Julia test artifacts..."
	@rm -f tests/generated_*.jl
	@rm -f tests/diff_*.jl
	@rm -f *.jl
	@echo "âœ… Julia artifacts cleaned"

# Quick test (for development)
quick-test:
	@echo "âš¡ Quick Julia Backend Test..."
	python3 -m pcs --code "sum(i*i for i in range(1, 10) if i%2==0)" --target julia --mode auto --parallel
	@echo "âœ… Quick test completed"

# Generate all golden files from fixture
regenerate-goldens:
	@echo "ðŸ”„ Regenerating Golden Files from Fixture..."
	@echo "âš ï¸  This will overwrite existing golden files!"
	@read -p "Continue? (y/N): " confirm && [ "$$confirm" = "y" ] || exit 1
	python3 scripts/regenerate_goldens.py
	@echo "âœ… Golden files regenerated"

# Show Julia backend status
status:
	@echo "ðŸ“Š Julia Backend Status:"
	@echo "========================"
	@echo "Python version: $$(python3 --version)"
	@echo "Julia version: $$(julia --version 2>/dev/null || echo 'Not installed')"
	@echo "PCS version: $$(python3 -m pcs --version 2>/dev/null || echo 'Not available')"
	@echo ""
	@echo "Test files:"
	@ls -la tests/fixtures/ 2>/dev/null || echo "  No fixtures found"
	@ls -la tests/golden/julia/ 2>/dev/null || echo "  No golden files found"
	@echo ""
	@echo "Scripts:"
	@ls -la scripts/test_*.py 2>/dev/null || echo "  No test scripts found"

# Benchmark targets
bench:
	@echo "ðŸš€ Running all backend benchmarks..."
	python3 scripts/bench_all.py

bench-agg:
	@echo "ðŸ“Š Aggregating benchmark results..."
	python3 scripts/aggregate_bench.py

bench-publish:
	@echo "ðŸ“ˆ Publishing benchmark dashboard..."
	@if command -v gh-pages >/dev/null 2>&1; then \
		gh-pages -d site; \
	else \
		echo "âš ï¸  gh-pages not found. Install with: npm install -g gh-pages"; \
		echo "   Or use GitHub Actions for automatic publishing"; \
	fi

bench-full: bench bench-agg
	@echo "ðŸŽ‰ Full benchmark suite completed!"

bench-status:
	@echo "ðŸ“Š Benchmark Status:"
	@echo "==================="
	@echo "Results directory:"
	@ls -la bench/results/ 2>/dev/null || echo "  No results found"
	@echo ""
	@echo "Site directory:"
	@ls -la site/ 2>/dev/null || echo "  No site files found"
	@echo ""
	@echo "Last benchmark:"
	@ls -t bench/results/*.ndjson 2>/dev/null | head -1 | xargs ls -la 2>/dev/null || echo "  No benchmarks found"