# Makefile for Polyglot Code Sampler - Julia Backend Dev Workflow

.PHONY: help test-julia test-fixtures test-differential benchmark-julia clean-julia dev-workflow

# Default target
help:
	@echo "üöÄ Polyglot Code Sampler - Julia Backend Dev Workflow"
	@echo "=================================================="
	@echo ""
	@echo "Available targets:"
	@echo "  test-julia      - Run all Julia backend tests"
	@echo "  test-fixtures   - Test IR fixture ‚Üí 4 goldens regression safety net"
	@echo "  test-differential - Run differential tests (IR ‚Üí Julia vs reference)"
	@echo "  benchmark-julia - Run Julia performance benchmarks"
	@echo "  dev-workflow    - Complete dev workflow (test + benchmark)"
	@echo "  clean-julia     - Clean Julia test artifacts"
	@echo ""
	@echo "Benchmark targets:"
	@echo "  bench           - Run all backend benchmarks (Julia, Rust, Go, TS, C#)"
	@echo "  bench-agg       - Aggregate benchmark results for dashboard"
	@echo "  bench-publish   - Publish benchmark dashboard to GitHub Pages"
	@echo "  bench-full      - Run benchmarks and aggregate results"
	@echo "  bench-status    - Show benchmark status and results"
	@echo ""
	@echo "Phase 2: Advanced benchmarking:"
	@echo "  bench-multi     - Run comprehensive multi-test benchmark suite"
	@echo "  trend-alerts    - Check for performance regressions with alerts"
	@echo "  bench-phase2    - Complete Phase 2 benchmark suite"
	@echo "  dashboard-preview - Preview enhanced dashboard locally"
	@echo ""

# Test Julia backend
test-julia:
	@echo "üß™ Running Julia Backend Tests..."
	python3 -m pytest tests/test_one_ir_many_goldens.py -v
	@echo "‚úÖ Julia backend tests completed"

# Test fixture golden files
test-fixtures:
	@echo "üß™ Testing IR Fixture ‚Üí 4 Goldens Regression Safety Net..."
	python3 scripts/test_fixture_goldens.py
	@echo "‚úÖ Fixture golden tests completed"

# Run differential tests
test-differential:
	@echo "üß™ Running Differential Tests (IR ‚Üí Julia vs Reference)..."
	python3 scripts/test_differential.py
	@echo "‚úÖ Differential tests completed"

# Run Julia performance benchmarks
benchmark-julia:
	@echo "‚ö° Running Julia Performance Benchmarks..."
	@if command -v julia >/dev/null 2>&1; then \
		echo "üìä Sequential vs Parallel Performance Comparison:"; \
		julia --project -e 'include("examples/julia/sanity_perf_check.jl")'; \
		echo "‚úÖ Julia benchmarks completed"; \
	else \
		echo "‚ö†Ô∏è  Julia not found - skipping benchmarks"; \
		echo "   Install Julia to run performance benchmarks"; \
	fi

# Complete dev workflow
dev-workflow: test-julia test-fixtures test-differential benchmark-julia
	@echo ""
	@echo "üéâ Complete Dev Workflow Completed!"
	@echo "‚úÖ All tests passed"
	@echo "‚úÖ Regression safety net intact"
	@echo "‚úÖ Performance benchmarks completed"
	@echo ""
	@echo "üöÄ Julia backend is ready for production!"

# Clean Julia test artifacts
clean-julia:
	@echo "üßπ Cleaning Julia test artifacts..."
	@rm -f tests/generated_*.jl
	@rm -f tests/diff_*.jl
	@rm -f *.jl
	@echo "‚úÖ Julia artifacts cleaned"

# Quick test (for development)
quick-test:
	@echo "‚ö° Quick Julia Backend Test..."
	python3 -m pcs --code "sum(i*i for i in range(1, 10) if i%2==0)" --target julia --mode auto --parallel
	@echo "‚úÖ Quick test completed"

# Generate all golden files from fixture
regenerate-goldens:
	@echo "üîÑ Regenerating Golden Files from Fixture..."
	@echo "‚ö†Ô∏è  This will overwrite existing golden files!"
	@read -p "Continue? (y/N): " confirm && [ "$$confirm" = "y" ] || exit 1
	python3 scripts/regenerate_goldens.py
	@echo "‚úÖ Golden files regenerated"

# Show Julia backend status
status:
	@echo "üìä Julia Backend Status:"
	@echo "========================"
	@echo "Python version: $$(python3 --version)"
	@echo "Julia version: $$(julia --version 2>/dev/null || echo 'Not installed')"
	@echo "PCS version: $$(python3 -m pcs --version 2>/dev/null || echo 'Not available')"
	@echo ""
	@echo "Test files:"
	@ls -la tests/fixtures/ 2>/dev/null || echo "  No fixtures found"
	@ls -la tests/golden/julia/ 2>/dev/null || echo "  No golden files found"
	@echo ""
	@echo "Scripts:"
	@ls -la scripts/test_*.py 2>/dev/null || echo "  No test scripts found"

# Benchmark targets
bench:
	@echo "üöÄ Running all backend benchmarks..."
	python3 scripts/bench_all.py

bench-agg:
	@echo "üìä Aggregating benchmark results..."
	python3 scripts/aggregate_bench.py

bench-publish:
	@echo "üìà Publishing benchmark dashboard..."
	@if command -v gh-pages >/dev/null 2>&1; then \
		gh-pages -d site; \
	else \
		echo "‚ö†Ô∏è  gh-pages not found. Install with: npm install -g gh-pages"; \
		echo "   Or use GitHub Actions for automatic publishing"; \
	fi

bench-full: bench bench-agg
	@echo "üéâ Full benchmark suite completed!"

bench-status:
	@echo "üìä Benchmark Status:"
	@echo "==================="
	@echo "Results directory:"
	@ls -la bench/results/ 2>/dev/null || echo "  No results found"
	@echo ""
	@echo "Site directory:"
	@ls -la site/ 2>/dev/null || echo "  No site files found"
	@echo ""
	@echo "Last benchmark:"
	@ls -t bench/results/*.ndjson 2>/dev/null | head -1 | xargs ls -la 2>/dev/null || echo "  No benchmarks found"

# Phase 2: Advanced benchmarking features
bench-multi:
	@echo "üöÄ Running multi-test benchmark suite..."
	python3 scripts/bench_multi_test.py

trend-alerts:
	@echo "üîç Checking for performance regressions..."
	python3 scripts/trend_alerts.py

bench-phase2: bench-multi bench-agg trend-alerts
	@echo "üéâ Phase 2 benchmark suite completed!"

# Enhanced dashboard features
dashboard-preview:
	@echo "üìà Previewing enhanced dashboard..."
	@if command -v python3 >/dev/null 2>&1; then \
		cd site && python3 -m http.server 8080; \
	else \
		echo "‚ö†Ô∏è  Python3 not found. Open site/index.html in your browser"; \
	fi
